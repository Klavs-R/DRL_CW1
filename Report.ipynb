{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ef8a88-2990-4c0b-90b5-cd447bbaa8d9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac272b-aca1-4255-beca-e5003252f032",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Navigation Project Report \n",
    "## The Algorithm \n",
    "The implementation used to solve this project was a Double Deep Q-Learning (DQL) agent. Standard DQL uses two Neural Networks (NN) and updates them (at different rates) through experience replay. The `local_NN` is updated more often and used to estimate the expected Q-values and the `target_NN` updates less often and is used as the target Q-values. Often the `target_NN` is simply set to $target_{NN} =  local_{NN}$ every n steps, however this implementation also implements a soft_update, where instead each n steps $target_{NN} = \\tau*local_{NN} + (1-\\tau)*target_{NN}$ where $\\tau$ is a hyperparameter. \n",
    "\n",
    "The difference between this and a Double DQL is solely in the calculation of the target values. Instead of using an `argmax()` over the outputs of the respective NNs to find the `target` and `expected` Q-values to compare, the Double DQL agent selects the \"best\" actions according solely to the `local_NN` which are then used to find the corresponding Q-values for those state, action pairs according to the `target_NN` to use as the target values.\n",
    "\n",
    "## Results\n",
    "Below is a list of all the parameters used (found through tests in `testing.ipynb` notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae94e41f-089f-4173-ba87-f19591871280",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_structure = [64, 64]   # NN with 2 64 node fully connected layers\n",
    "gamma = 0.9               # Best gamma from testin\n",
    "tau = 0.001               # Best Tau from testing\n",
    "batch_n = 64              # Number of events to replay at each learning batch\n",
    "learning_rate = 5e-4      # Default Learning rate\n",
    "update_every = 4          # How often to replay events "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c93b6f-3a5e-4bba-8b4e-a0eeba68f930",
   "metadata": {},
   "source": [
    "Initialising an agent with these parameters and training, we get the agent saved in `model_weights`. The rewards plot from this training is shown below. \n",
    "\n",
    "<img src=\"ouput_rewards.png\">\n",
    "\n",
    "The trained agent easily achieves a score of above 12 almost every time it is run, as can be seen in the snippet below, where the fully trained agent is allowed to run. \n",
    "\n",
    "<img src=\"banana_gif.gif\">\n",
    "\n",
    "To see a live running of the trained aent, you may run the final cells in `rtain_agent.ipynb`.\n",
    "\n",
    "## Future Work \n",
    "There is still plenty that can be done to improve the model as it stands, through more indepth parameter tuning, as I only mainly considered the structure, $\\gamma$ and $\\tau$. \n",
    "\n",
    "Another way to possibly imporve the performace of this agent would be to include prioritised experience replay, which includes giving each experience in the replay buffer a weight inversesly proportional to how well the agent did on that particular experience when it was last seen. These weights are then turned into probabilities, whih are used to sample the next batch. This makes it more likely that experiences the agent struggles with will come up again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr_cw1",
   "language": "python",
   "name": "dr_cw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
